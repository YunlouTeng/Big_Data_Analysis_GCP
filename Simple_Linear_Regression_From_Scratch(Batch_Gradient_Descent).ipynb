{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ogl_xYIv_dRLn46hEGFmwedtHDdgbQOD",
      "authorship_tag": "ABX9TyMyaSWAVs75N4tyqeN/Zv2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YunlouTeng/Big_Data_Analysis_GCP/blob/main/Simple_Linear_Regression_From_Scratch(Batch_Gradient_Descent).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm-9dCyunIHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c53a7fa-78d1-4d07-a3f9-f7f2f3619a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 43 kB/s \n",
            "\u001b[K     |████████████████████████████████| 199 kB 49.3 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#pip install --ignore-installed -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import sys\n",
        "from operator import add\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: wordcount <file> <output> \", file=sys.stderr)\n",
        "        exit(-1)"
      ],
      "metadata": {
        "id": "HnnFKquX18vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "from pyspark import SparkConf,SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as func\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "metadata": {
        "id": "PZF_U1Pvx5CS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0088c541-d76c-4cd6-c86f-6c3ec7341cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data= sys.argv[1]\n",
        "#data = 'taxi-data-sorted-small.csv.bz2'"
      ],
      "metadata": {
        "id": "muuhxsDczeCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType() \\\n",
        "      .add(\"Taxi_id\",StringType(),True) \\\n",
        "      .add(\"Driver_id\",StringType(),True) \\\n",
        "      .add(\"pickup_datetime\",TimestampType(),True) \\\n",
        "      .add(\"dropoff_datetime\",TimestampType(),True) \\\n",
        "      .add(\"trip_time_in_secs\",ShortType(),True) \\\n",
        "      .add(\"trip_distance\",FloatType(),True) \\\n",
        "      .add(\"pickup_longitude\",FloatType(),True) \\\n",
        "      .add(\"pickup_latitude\",FloatType(),True) \\\n",
        "      .add(\"dropoff_longitude\",FloatType(),True) \\\n",
        "      .add(\"dropoff_latitude\",FloatType(),True) \\\n",
        "      .add(\"payment_type\",StringType(),True) \\\n",
        "      .add(\"fare_amount\",FloatType(),True) \\\n",
        "      .add(\"surcharge\",FloatType(),True) \\\n",
        "      .add(\"mta_tax\",FloatType(),True) \\\n",
        "      .add(\"tip_amount\",FloatType(),True) \\\n",
        "      .add(\"toll_amount\",FloatType(),True) \\\n",
        "      .add(\"toal_amount\",FloatType(),True)"
      ],
      "metadata": {
        "id": "UdthysM39yY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taxi = spark.read.format(\"csv\") \\\n",
        "      .schema(schema) \\\n",
        "      .load(data)"
      ],
      "metadata": {
        "id": "JJj8xSs-zeLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Clean-up Step"
      ],
      "metadata": {
        "id": "VdZehMDS1lS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove all taxi rides that are less than 2 mins or more than 1 hour\n",
        "taxi = taxi.filter((taxi.trip_time_in_secs > 120) & (taxi.trip_time_in_secs < 3600))\n",
        "#Remove\tall\ttaxi\trides\tthat\thave\t”fare\tamount”\tless\tthan\t3\tdollars or\tmore\tthan\t200\tdollars\n",
        "taxi = taxi.filter((taxi.fare_amount > 3) & (taxi.fare_amount < 200))\n",
        "#Remove\tall\ttaxi\trides\tthat\thave\t”trip distance”\tless\tthan\t1\tmile\tor\tmore\tthan\t50\tmiles\n",
        "taxi = taxi.filter((taxi.trip_distance > 1) & (taxi.trip_distance < 50))\n",
        "#Remove\tall\ttaxi\trides\tthat\thave\t”tolls\tamount”\tless\tthan\t3\tdollars.\n",
        "taxi = taxi.filter(taxi.toll_amount > 3)"
      ],
      "metadata": {
        "id": "UwHdqeswzw74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(taxi.select(\"trip_distance\").collect())\n",
        "y = np.array(taxi.select(\"fare_amount\").collect())\n",
        "\n",
        "#scale the data\n",
        "X = (X - X.mean()) / X.std()"
      ],
      "metadata": {
        "id": "UkSF8QKXakKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinReg:\n",
        "    \n",
        "    # Initializing lr: learning rate, epochs: no. of iterations, \n",
        "    # weights & bias: parameters as None\n",
        "    # default lr: 0.0001, epochs: 100\n",
        "    def __init__(self, lr=0.0001, epochs=100):\n",
        "      \n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "    # Training function: fit\n",
        "    def fit(self, X, y):\n",
        "        # shape of X: (number of training examples: m, number of    \n",
        "        # features: n)\n",
        "        m, n = X.shape\n",
        "        # Initializing weights as a matrix of zeros of size: (number\n",
        "        # of features: n, 1) and bias as 0\n",
        "        self.weights = np.zeros((n,1))\n",
        "        self.bias = 0.1\n",
        "        \n",
        "        # reshaping y as (m,1) in case your dataset initialized as \n",
        "        # (m,) which can cause problems\n",
        "        y = y.reshape(m,1)\n",
        "        \n",
        "        # empty lsit to store losses so we can plot them later \n",
        "        # against epochs\n",
        "        losses = []\n",
        "        weights = []\n",
        "        bias = []\n",
        "        \n",
        "        # Gradient Descent loop/ Training loop\n",
        "        for epoch in range(self.epochs):\n",
        "\n",
        "            weights.append(self.weights)\n",
        "            bias.append(self.bias)\n",
        "        \n",
        "            # Calculating prediction: y_hat or h(x)\n",
        "            y_hat = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "     \n",
        "            # Calculting loss\n",
        "            loss = np.sum((y_hat - y)**2)\n",
        "    \n",
        "            # Appending loss in list: losses\n",
        "            losses.append(loss)\n",
        "            \n",
        "    \n",
        "            # Calculating derivatives of parameters(weights, and \n",
        "            # bias) \n",
        "            dw = (1/m)*np.dot(X.T, (y_hat - y))\n",
        "            db = (1/m)*np.sum((y_hat - y))\n",
        "   # Updating the parameters: parameter := parameter - lr*derivative\n",
        "   # of loss/cost w.r.t parameter)\n",
        "            \n",
        "            self.bias -= self.lr*db\n",
        "            self.weights -= self.lr*dw\n",
        "        # returning the parameter so we can look at them later\n",
        "        return self.weights, self.bias, weights, bias, losses\n",
        "    # Predicting(calculating y_hat with our updated weights) for the \n",
        "    # testing/validation     \n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias"
      ],
      "metadata": {
        "id": "Fm8xlICvxIzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinReg(epochs=100)"
      ],
      "metadata": {
        "id": "XZszoCiDbQyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_w, final_b, list_w, list_b, list_lost = model.fit(X,y)"
      ],
      "metadata": {
        "id": "o11THKkSbQ6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list_w)"
      ],
      "metadata": {
        "id": "S_6dIUBrbRJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list_b)"
      ],
      "metadata": {
        "id": "IgYeOF10csLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list_lost)"
      ],
      "metadata": {
        "id": "zx8zM34Fcsy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YCY0nCADy1kI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}